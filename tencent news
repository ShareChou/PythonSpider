# -*- coding:utf-8 -*-
import urllib2
import re
import os

#去掉字符类的html操作符
def filter_tags(htmlstr):
   re_cdata=re.compile('//<!\[CDATA\[[^>]*//\]\]>',re.I)
   re_script=re.compile('<\s*script[^>]*>[^<]*<\s*/\s*script\s*>',re.I)
   re_style=re.compile('<\s*style[^>]*>[^<]*<\s*/\s*style\s*>',re.I)
   re_p=re.compile('<P\s*?/?>')
   re_h=re.compile('</?\w+[^>]*>')
   re_comment=re.compile('<!--[^>]*-->')
   s=re_cdata.sub('',htmlstr)
   s=re_script.sub('',s)
   s=re_style.sub('',s)
   s=re_p.sub('\r\n',s)
   s=re_h.sub('',s)
   s=re_comment.sub('',s)
   blank_line=re.compile('\n+')
   s=blank_line.sub('\n',s)
   return s

#获取超链接
urlcontent = urllib2.urlopen('http://news.qq.com/a/20120506/').read()
#print urlcontent.decode('gbk')
rege = re.compile(r"/a/\d{8}/\d{6}.htm")#compile把正则表达式编译成正则表达式对象
get_url = re.findall(rege,urlcontent)
#print get_url #列表list格式
get_url.sort()#排序网页顺序

#打印完整链接
for i in xrange(0,len(get_url)):
    get_url[i] = "http://news.qq.com"+ get_url[i]
    try:
        sub_web = urllib2.urlopen(get_url[i]).read()
    except urllib2.URLError,e:
        print "获取链接失败，跳过"
        continue
    #匹配标题+正文
    re_keyt = "<h1>(.*?)</h1>"
    title = re.findall(re_keyt,sub_web)
    #print title[0].decode('gbk')
    re_keyc = re.compile("<div id=\"C-Main-Article-QQ\".*</p></div>",re.DOTALL)#匹配出
    content = re.findall(re_keyc,sub_web)
    if len(title) == 0 or len(content) == 0:
        continue
    re_content = filter_tags(title[0]+"\r\n"+content[0])
    #print re_content.decode('gbk')
    w = file('tencentnews.txt','a+')
    w.write(re_content)
    w.close()
print '全部下载完成'

