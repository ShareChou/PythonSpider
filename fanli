# -*- coding:utf-8 -*-
import urllib2
import re
from bs4 import BeautifulSoup
import time
import socket
import sys
reload(sys)
#sys.setdefaultencoding('utf-8') #输出的内容
fanli_url = "http://zhide.fanli.com/p"
format_url = "http://zhide.fanli.com/detail/1-" #商品地址

class fanli():
    def __init__(self):
        self.user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0'
        self.html_data = []

    def get_html(self,start_page=1,end_page=1):
        for i in range(start_page,end_page+1):
            print ("----正在采集第%d页的数据"%i)
            rt = urllib2.Request(fanli_url+str(i))#拼接
            rt.add_header('User-Agent',self.user_agent)#添加，提交头部信息进行验证
            try:
                my_data = urllib2.urlopen(rt).read()#读取源代码
               #print my_data#打印原码
                self.html_data.append(my_data)
                time.sleep(2)#防止ip被封
                socket.setdefaulttimeout(1)#控制下载内容时等待的时间
            except urllib2.URLError,e:
                if hasattr(e,'reason'):
                    print u"连接失败",e.reason
        return str(self.html_data)

#html = fanli().get_html()

#获取商品的超链接
class GetData():
    def __init__(self):
        self.html = fanli().get_html()#调用上面函数
        self.href = []#放置超链接后6位数字
        self.ls = [] #去掉重复
        self.url = []

    def get_hrefurl(self):
        reg = r'data-id="\d{6}"'
        result = re.compile(reg)
        tag = result.findall(self.html)#匹配
       # print tag
        for i in tag:
            self.href.append(i)
            #print self.href
        reg2 = r"\d{6}"
        result2 = re.findall(reg2,str(self.href))
        if len(result2):
            for data in result2:
                if data not in self.ls:
                    self.ls.append(data)
                    #print data 6位数字
                    url = format_url+str(data)
                    self.url.append(url)
                   # print self.url
        return self.url

#获取商品的信息
class Href_mg():
    def __init__(self):
        self.list = GetData().get_hrefurl()#获取商品链接
        self.txt_list = []#页面信息如返利信息
    def show_mg(self):
        for item in range(len(self.list)):
            url = str(self.list[item])
            try:
                mg = urllib2.Request(url)
                req = urllib2.urlopen(mg).read()
                soup = BeautifulSoup(req,"html.parser")
                txt = soup.find_all('h1')
                self.txt_list.append(txt)
               #print self.txt_list
            except urllib2.URLError,e:
                print e.reason
        return str(self.txt_list)
#data = Href_mg().show_mg()
if __name__ == "__main__":#判断文件入口
    path = "yaozhi.txt"
    with open(path,'a') as file:
        data = Href_mg().show_mg()
        #print data
        reg4 = r'<.+?>'
        data_s = re.sub(reg4,' ',data).replace('全网最低', '').replace('[', '').replace(']', '').replace(',','\n').strip().replace( '  ', '').decode('utf-8').encode('Unicode')
        print data_s
        file.write(data_s)
